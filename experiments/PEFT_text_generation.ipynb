{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install torch==2.1.0\n",
    "#%pip install transformers==4.31.0\n",
    "#%pip install sentence-transformers==2.2.2\n",
    "#%pip install pinecone-client==2.2.2\n",
    "#%pip install datasets==2.14.0\n",
    "#%pip install langchain==0.0.240\n",
    "#%pip install bitsandbytes==0.41.0\n",
    "#%pip install -q -U trl git+https://github.com/huggingface/peft.git\n",
    "#%pip install -U datasets\n",
    "#%pip install -U peft trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "whole_dataset = load_dataset('json', data_files='/root/llama/datasets/preprocessed_dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "# Cleaning function definition\n",
    "def clean_text(example):\n",
    "    # Replace newline characters with space\n",
    "    example['title'] = example['title'].replace('\\n', ' ')\n",
    "    example['summary'] = example['summary'].replace('\\n', ' ')\n",
    "    # Replace apostrophes with empty string\n",
    "    example['title'] = example['title'].replace('\\'', '')\n",
    "    example['summary'] = example['summary'].replace('\\'', '')\n",
    "    return example\n",
    "\n",
    "# Application of the cleaning function\n",
    "cleaned_dataset = whole_dataset['train'].map(clean_text)\n",
    "print(cleaned_dataset[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "# First split\n",
    "train_test_split = cleaned_dataset.train_test_split(test_size=0.20)\n",
    "train1_dataset = train_test_split['train']\n",
    "remaining1_dataset = train_test_split['test']\n",
    "\n",
    "# Second split\n",
    "eval_test_split = remaining1_dataset.train_test_split(test_size=0.5)\n",
    "eval1_dataset = eval_test_split['train']\n",
    "test1_dataset = eval_test_split['test']\n",
    "\n",
    "# Combine datasets into a DatasetDict\n",
    "split_datasets = DatasetDict({\n",
    "    'train': train1_dataset,\n",
    "    'validation': eval1_dataset,\n",
    "    'test': test1_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove '<s>' token\n",
    "    text = text.replace('<s>', '')\n",
    "    # Replace double spaces with single spaces\n",
    "    while \"  \" in text:\n",
    "        text = text.replace(\"  \", \" \")\n",
    "    return text.strip()  # Strip leading and trailing whitespace\n",
    "\n",
    "final_train1_dataset = train1_dataset.map(lambda item: {'title': clean_text(item['title']),\n",
    "                                                                    'summary': clean_text(item['summary'])})\n",
    "final_eval1_dataset = eval1_dataset.map(lambda item: {'title': clean_text(item['title']),\n",
    "                                                                  'summary': clean_text(item['summary'])})\n",
    "final_test1_dataset = test1_dataset.map(lambda item: {'title': clean_text(item['title']),\n",
    "                                                                  'summary': clean_text(item['summary'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the processed and segmented dataset from the saved JSON file\n",
    "dataset = load_dataset('json', data_files='/root/llama/datasets/small_segmented_subset_papers.json', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "# First split\n",
    "train_test_split = dataset.train_test_split(test_size=0.5)\n",
    "train_dataset = train_test_split['train']\n",
    "remaining_dataset = train_test_split['test']\n",
    "\n",
    "# Second split\n",
    "eval_test_split = remaining_dataset.train_test_split(test_size=0.5)\n",
    "eval_dataset = eval_test_split['train']\n",
    "test_dataset = eval_test_split['test']\n",
    "\n",
    "# Combine datasets into a DatasetDict\n",
    "split_datasets = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': eval_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_summary(example, tokenizer, max_length, overlap):\n",
    "    tokens = tokenizer.encode(example['summary'], add_special_tokens=True)\n",
    "    total_tokens = len(tokens)\n",
    "    start = 0\n",
    "    segments = []\n",
    "\n",
    "    while start < total_tokens:\n",
    "        end = start + max_length\n",
    "        segment = tokens[start:end]\n",
    "        segments.append(tokenizer.decode(segment))\n",
    "        start = end - overlap\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this was done once, when creating the segmented dataset, once we create the dataset, this step should be skipped\n",
    "\n",
    "def create_segmented_dataset(dataset, tokenizer, max_length, overlap):\n",
    "    segmented_dataset = []\n",
    "\n",
    "    for item in dataset:\n",
    "        summary_segments = sliding_window_summary(item, tokenizer, max_length, overlap)\n",
    "        for segment in summary_segments:\n",
    "            new_item = {'title': item['title'], 'summary_segment': segment}\n",
    "            segmented_dataset.append(new_item)\n",
    "\n",
    "    return segmented_dataset\n",
    "\n",
    "max_seq_length=200\n",
    "overlap=20\n",
    "\n",
    "# Apply it to the train, test, and eval datasets\n",
    "segmented_train_dataset = create_segmented_dataset(train_dataset, tokenizer, max_seq_length, overlap)\n",
    "segmented_test_dataset = create_segmented_dataset(test_dataset, tokenizer, max_seq_length, overlap)\n",
    "segmented_eval_dataset = create_segmented_dataset(eval_dataset, tokenizer, max_seq_length, overlap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "segmented_train_dataset = Dataset.from_pandas(pd.DataFrame(segmented_train_dataset))\n",
    "segmented_test_dataset = Dataset.from_pandas(pd.DataFrame(segmented_test_dataset))\n",
    "segmented_eval_dataset = Dataset.from_pandas(pd.DataFrame(segmented_eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove '<s>' token\n",
    "    text = text.replace('<s>', '')\n",
    "    # Replace double spaces with single spaces\n",
    "    while \"  \" in text:\n",
    "        text = text.replace(\"  \", \" \")\n",
    "    return text.strip()  # Strip leading and trailing whitespace\n",
    "\n",
    "segmented_train_dataset = segmented_train_dataset.map(lambda item: {'title': clean_text(item['title']),\n",
    "                                                                    'summary_segment': clean_text(item['summary_segment'])})\n",
    "segmented_eval_dataset = segmented_eval_dataset.map(lambda item: {'title': clean_text(item['title']),\n",
    "                                                                  'summary_segment': clean_text(item['summary_segment'])})\n",
    "segmented_test_dataset = segmented_test_dataset.map(lambda item: {'title': clean_text(item['title']),\n",
    "                                                                  'summary_segment': clean_text(item['summary_segment'])})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell is a demonstration of how we saved the processed and segmented dataset, should be skipped\n",
    "import pandas as pd\n",
    "#Convert the dataset to a pandas DataFrame\n",
    "train_df = pd.DataFrame(segmented_train_dataset)\n",
    "# Saving the DataFrame to a JSON file\n",
    "train_df.to_json('/root/llama/datasets/small_segmented_subset_papers.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_parameters = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir = \"./results\"\n",
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "optim = \"paged_adamw_32bit\"\n",
    "save_steps = 100\n",
    "logging_steps = 50\n",
    "learning_rate = 2e-4\n",
    "max_grad_norm = 0.4\n",
    "#max_steps = 100\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    evaluation_strategy=\"steps\", \n",
    "    eval_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=True,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    #max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 400\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset, \n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_parameters,\n",
    "    dataset_text_field=\"summary_segment\",  \n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_wandb = False\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(\"outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig.from_pretrained('outputs')\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = \"What is the Big Bang theory? ### Assistant:\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=400)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def condense_summary(summary):\n",
    "    # Remove headings and numbers\n",
    "    summary = re.sub(r'(\\n### .+\\n)|(\\d+\\.\\s)', '', summary)\n",
    "\n",
    "    # Split into sentences\n",
    "    sentences = re.split(r'\\.\\s+', summary)\n",
    "\n",
    "    # Remove duplicate sentences\n",
    "    seen = set()\n",
    "    unique_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if sentence not in seen:\n",
    "            unique_sentences.append(sentence)\n",
    "            seen.add(sentence)\n",
    "\n",
    "    # Join sentences into a paragraph\n",
    "    condensed_summary = '. '.join(unique_sentences)\n",
    "\n",
    "    return condensed_summary.strip()\n",
    "\n",
    "num_titles_to_generate = 10  # Set the number of titles you want to generate summaries for\n",
    "\n",
    "prompts = []\n",
    "for index, data in enumerate(train_dataset):\n",
    "    if index >= num_titles_to_generate:\n",
    "        break  \n",
    "    title = data['title']\n",
    "    prompt = f\"The following title is from a research paper in the fields of Machine Learning (ML), Artificial Intelligence (AI) and related subfields. Please provide a concise summary in an academic style that covers the main points and conclusions for the paper titled '{title}'. Directly start with the summary.\"\n",
    "    prompts.append(prompt)\n",
    "\n",
    "# Assuming you have a list 'actual_summaries' corresponding to these titles\n",
    "model_outputs = []\n",
    "for prompt in prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=400)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Post-processing to remove the echoed prompt\n",
    "    if generated_text.startswith(prompt):\n",
    "        generated_text = generated_text[len(prompt):].strip()\n",
    "\n",
    "    # Condense the generated text\n",
    "    condensed_text = condense_summary(generated_text)\n",
    "\n",
    "    model_outputs.append(condensed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we created a title-to-summary mapping from the dataset with whole summaries, which we'll use for evaluation later\n",
    "title_to_summary_mapping = {data['title']: data['summary'] for data in final_train1_dataset}\n",
    "\n",
    "# Now we generate summaries based on titles from segmented dataset\n",
    "generated_summaries = []\n",
    "actual_summaries = []\n",
    "def condense_summary(summary):\n",
    "    # Removing headings and numbers\n",
    "    summary = re.sub(r'(\\n### .+\\n)|(\\d+\\.\\s)', '', summary)\n",
    "\n",
    "    # Splitting into sentences\n",
    "    sentences = re.split(r'\\.\\s+', summary)\n",
    "\n",
    "    # Removal of duplicate sentences\n",
    "    seen = set()\n",
    "    unique_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if sentence not in seen:\n",
    "            unique_sentences.append(sentence)\n",
    "            seen.add(sentence)\n",
    "\n",
    "    # Joining sentences into a paragraph\n",
    "    condensed_summary = '. '.join(unique_sentences)\n",
    "\n",
    "    return condensed_summary.strip()\n",
    "\n",
    "num_titles_to_generate = 20  \n",
    "prompts = []\n",
    "for index, data in enumerate(train_dataset):\n",
    "    if index >= num_titles_to_generate:\n",
    "        break  \n",
    "    title = data['title']\n",
    "    prompt = f\"The following title is from a research paper in the fields of Machine Learning (ML), Artificial Intelligence (AI) and related subfields. Please provide a concise summary in an academic style that covers the main points and conclusions for the paper titled '{title}'. Directly start with the summary.\"\n",
    "    prompts.append(prompt)\n",
    "\n",
    "# Using the 'actual_summaries' corresponding to these titles\n",
    "for prompt in prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=400)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Post-processing to remove the echoed prompt\n",
    "    if generated_text.startswith(prompt):\n",
    "        generated_text = generated_text[len(prompt):].strip()\n",
    "\n",
    "    condensed_text = condense_summary(generated_text)\n",
    "    generated_summaries.append(condensed_text)\n",
    "\n",
    "    # Fetch the actual summary from train1_dataset based on the title\n",
    "    actual_summary = title_to_summary_mapping.get(title, \"No matching summary found\")\n",
    "    actual_summaries.append(actual_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_index = 0  # Lets test and view results for one title at a time\n",
    "if specific_index < len(prompts):\n",
    "    print(f\"Prompt:\\n{prompts[specific_index]}\\n\")\n",
    "    print(f\"Generated Summary:\\n{generated_summaries[specific_index]}\\n\")\n",
    "    print(f\"Actual Summary:\\n{actual_summaries[specific_index]}\\n\")\n",
    "else:\n",
    "    print(f\"No data available for index {specific_index}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_bleu_scores(actual_summaries, generated_summaries):\n",
    "    # Calculating BLEU scores for each summary pair\n",
    "    smoothie = SmoothingFunction().method2\n",
    "    bleu_scores = [sentence_bleu([actual.split()], generated.split(), smoothing_function=smoothie) for actual, generated in zip(actual_summaries, generated_summaries)]\n",
    "    return bleu_scores\n",
    "\n",
    "def calculate_rouge_scores(actual_summaries, generated_summaries):\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = rouge.get_scores(generated_summaries, actual_summaries, avg=True)\n",
    "    return rouge_scores\n",
    "\n",
    "bleu_scores = calculate_bleu_scores(actual_summaries, generated_summaries)\n",
    "avg_bleu_score = np.mean(bleu_scores)\n",
    "rouge_scores = calculate_rouge_scores(actual_summaries, generated_summaries)\n",
    "\n",
    "# Extracting average F1 scores for each ROUGE metric\n",
    "rouge_1_f1 = rouge_scores['rouge-1']['f']\n",
    "rouge_2_f1 = rouge_scores['rouge-2']['f']\n",
    "rouge_l_f1 = rouge_scores['rouge-l']['f']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "metrics = ['BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
    "scores = [avg_bleu_score, rouge_1_f1, rouge_2_f1, rouge_l_f1]\n",
    "\n",
    "plt.bar(metrics, scores, color=['cyan', 'blue', 'green', 'red'])\n",
    "plt.title('Average BLEU and ROUGE Scores')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1) \n",
    "plt.show()\n",
    "\n",
    "# Printing BLEU and ROUGE scores\n",
    "print(f\"Average BLEU Score: {avg_bleu_score:.4f}\")\n",
    "print(f\"Average ROUGE-1 F1 Score: {rouge_1_f1:.4f}\")\n",
    "print(f\"Average ROUGE-2 F1 Score: {rouge_2_f1:.4f}\")\n",
    "print(f\"Average ROUGE-L F1 Score: {rouge_l_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newpeft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

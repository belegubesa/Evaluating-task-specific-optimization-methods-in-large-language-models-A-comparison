{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install torch==2.1.0\n",
    "#%pip install transformers==4.31.0\n",
    "#%pip install sentence-transformers==2.2.2\n",
    "#%pip install pinecone-client==2.2.2\n",
    "#%pip install datasets==2.14.0\n",
    "#%pip install langchain==0.0.240\n",
    "#%pip install bitsandbytes==0.41.0\n",
    "#%pip install -q -U trl git+https://github.com/huggingface/peft.git\n",
    "#%pip install -U datasets\n",
    "#%pip install -U peft trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba90654d3ca418c927cc3dffbd56751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/PEFT_llama_papers.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdeathstar/root/PEFT_llama_papers.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmeta-llama/Llama-2-7b-chat-hf\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdeathstar/root/PEFT_llama_papers.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m bnb_config \u001b[39m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdeathstar/root/PEFT_llama_papers.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     load_in_4bit\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdeathstar/root/PEFT_llama_papers.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     bnb_4bit_quant_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnf4\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdeathstar/root/PEFT_llama_papers.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     bnb_4bit_compute_dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat16,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeathstar/root/PEFT_llama_papers.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdeathstar/root/PEFT_llama_papers.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeathstar/root/PEFT_llama_papers.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     model_name,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeathstar/root/PEFT_llama_papers.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     quantization_config\u001b[39m=\u001b[39;49mbnb_config,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeathstar/root/PEFT_llama_papers.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeathstar/root/PEFT_llama_papers.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeathstar/root/PEFT_llama_papers.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_cache \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeathstar/root/PEFT_llama_papers.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name, trust_remote_code\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/newpeft/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    567\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    568\u001b[0m     )\n\u001b[1;32m    569\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n",
      "File \u001b[0;32m~/newpeft/lib/python3.9/site-packages/transformers/modeling_utils.py:3850\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3841\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3842\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3843\u001b[0m     (\n\u001b[1;32m   3844\u001b[0m         model,\n\u001b[1;32m   3845\u001b[0m         missing_keys,\n\u001b[1;32m   3846\u001b[0m         unexpected_keys,\n\u001b[1;32m   3847\u001b[0m         mismatched_keys,\n\u001b[1;32m   3848\u001b[0m         offload_index,\n\u001b[1;32m   3849\u001b[0m         error_msgs,\n\u001b[0;32m-> 3850\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   3851\u001b[0m         model,\n\u001b[1;32m   3852\u001b[0m         state_dict,\n\u001b[1;32m   3853\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3854\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3855\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3856\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   3857\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   3858\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   3859\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   3860\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   3861\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   3862\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   3863\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   3864\u001b[0m         is_quantized\u001b[39m=\u001b[39;49m(\u001b[39mgetattr\u001b[39;49m(model, \u001b[39m\"\u001b[39;49m\u001b[39mquantization_method\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m) \u001b[39m==\u001b[39;49m QuantizationMethod\u001b[39m.\u001b[39;49mBITS_AND_BYTES),\n\u001b[1;32m   3865\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   3866\u001b[0m     )\n\u001b[1;32m   3868\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_4bit \u001b[39m=\u001b[39m load_in_4bit\n\u001b[1;32m   3869\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_8bit \u001b[39m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m~/newpeft/lib/python3.9/site-packages/transformers/modeling_utils.py:4284\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   4280\u001b[0m                     set_module_quantized_tensor_to_device(\n\u001b[1;32m   4281\u001b[0m                         model_to_load, key, \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39mempty(\u001b[39m*\u001b[39mparam\u001b[39m.\u001b[39msize(), dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m   4282\u001b[0m                     )\n\u001b[1;32m   4283\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 4284\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[39m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[1;32m   4285\u001b[0m             model_to_load,\n\u001b[1;32m   4286\u001b[0m             state_dict,\n\u001b[1;32m   4287\u001b[0m             loaded_keys,\n\u001b[1;32m   4288\u001b[0m             start_prefix,\n\u001b[1;32m   4289\u001b[0m             expected_keys,\n\u001b[1;32m   4290\u001b[0m             device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   4291\u001b[0m             offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   4292\u001b[0m             offload_index\u001b[39m=\u001b[39;49moffload_index,\n\u001b[1;32m   4293\u001b[0m             state_dict_folder\u001b[39m=\u001b[39;49mstate_dict_folder,\n\u001b[1;32m   4294\u001b[0m             state_dict_index\u001b[39m=\u001b[39;49mstate_dict_index,\n\u001b[1;32m   4295\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   4296\u001b[0m             is_quantized\u001b[39m=\u001b[39;49mis_quantized,\n\u001b[1;32m   4297\u001b[0m             is_safetensors\u001b[39m=\u001b[39;49mis_safetensors,\n\u001b[1;32m   4298\u001b[0m             keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   4299\u001b[0m             unexpected_keys\u001b[39m=\u001b[39;49munexpected_keys,\n\u001b[1;32m   4300\u001b[0m         )\n\u001b[1;32m   4301\u001b[0m         error_msgs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_error_msgs\n\u001b[1;32m   4302\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/newpeft/lib/python3.9/site-packages/transformers/modeling_utils.py:839\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, is_quantized, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[1;32m    830\u001b[0m             set_module_quantized_tensor_to_device(\n\u001b[1;32m    831\u001b[0m                 model,\n\u001b[1;32m    832\u001b[0m                 param_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    835\u001b[0m                 quantized_stats\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mSCB\u001b[39m\u001b[39m\"\u001b[39m: state_dict[fp16_statistics_key]},\n\u001b[1;32m    836\u001b[0m             )\n\u001b[1;32m    837\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    838\u001b[0m         \u001b[39m# loading not quantized params in quantized model\u001b[39;00m\n\u001b[0;32m--> 839\u001b[0m         set_module_quantized_tensor_to_device(model, param_name, param_device, value\u001b[39m=\u001b[39;49mparam)\n\u001b[1;32m    841\u001b[0m \u001b[39mreturn\u001b[39;00m error_msgs, offload_index, state_dict_index\n",
      "File \u001b[0;32m~/newpeft/lib/python3.9/site-packages/transformers/integrations/bitsandbytes.py:121\u001b[0m, in \u001b[0;36mset_module_quantized_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, quantized_stats)\u001b[0m\n\u001b[1;32m    113\u001b[0m                 new_value \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mParams4bit\u001b[39m.\u001b[39mfrom_prequantized(\n\u001b[1;32m    114\u001b[0m                     data\u001b[39m=\u001b[39mnew_value,\n\u001b[1;32m    115\u001b[0m                     quantized_stats\u001b[39m=\u001b[39mquantized_stats,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m                     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    119\u001b[0m                 )\n\u001b[1;32m    120\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 121\u001b[0m                 new_value \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mParams4bit(new_value, requires_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m    122\u001b[0m         module\u001b[39m.\u001b[39m_parameters[tensor_name] \u001b[39m=\u001b[39m new_value\n\u001b[1;32m    124\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/newpeft/lib/python3.9/site-packages/bitsandbytes/nn/modules.py:179\u001b[0m, in \u001b[0;36mParams4bit.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m device, dtype, non_blocking, convert_to_format \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39m_parse_to(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    178\u001b[0m \u001b[39mif\u001b[39;00m (device \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m device\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mdevice\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 179\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcuda(device)\n\u001b[1;32m    180\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquant_state\n",
      "File \u001b[0;32m~/newpeft/lib/python3.9/site-packages/bitsandbytes/nn/modules.py:156\u001b[0m, in \u001b[0;36mParams4bit.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m, device):\n\u001b[0;32m--> 156\u001b[0m     w \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mcontiguous()\u001b[39m.\u001b[39;49mhalf()\u001b[39m.\u001b[39;49mcuda(device)\n\u001b[1;32m    157\u001b[0m     w_4bit, quant_state \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mquantize_4bit(w, blocksize\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocksize, compress_statistics\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompress_statistics, quant_type\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquant_type)\n\u001b[1;32m    158\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m w_4bit\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "whole_dataset = load_dataset('json', data_files='/root/llama/datasets/preprocessed_dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summary': ['we propose an architecture for vqa which utilizes recurrent layers to generate visual and textual attention the memory characteristic of the proposed recurrent attention units offers a rich joint embedding of visual and textual features and enables the model to reason relations between several parts of the image and question our single model outperforms the first place winner on the vqa 10 dataset performs within margin to the current stateoftheart ensemble model we also experiment with replacing attention mechanisms in other stateoftheart models with our implementation and show increased accuracy in both cases our recurrent attention mechanism improves performance in tasks requiring sequential or relational reasoning on the vqa dataset', 'recent approaches based on artificial neural networks anns have shown promising results for shorttext classification however many short texts occur in sequences eg sentences in a document or utterances in a dialog and most existing annbased systems do not leverage the preceding short texts when classifying a subsequent one in this work we present a model based on recurrent neural networks and convolutional neural networks that incorporates the preceding short texts our model achieves stateoftheart results on three different datasets for dialog act prediction'], 'title': ['dual recurrent attention units for visual question answering', 'sequential shorttext classification with recurrent and convolutional   neural networks']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "# Cleaning function definition\n",
    "def clean_text(example):\n",
    "    # Replace newline characters with space\n",
    "    example['title'] = example['title'].replace('\\n', ' ')\n",
    "    example['summary'] = example['summary'].replace('\\n', ' ')\n",
    "    # Replace apostrophes with empty string\n",
    "    example['title'] = example['title'].replace('\\'', '')\n",
    "    example['summary'] = example['summary'].replace('\\'', '')\n",
    "    return example\n",
    "\n",
    "# Application of the cleaning function\n",
    "cleaned_dataset = whole_dataset['train'].map(clean_text)\n",
    "print(cleaned_dataset[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "# First split\n",
    "train_test_split = cleaned_dataset.train_test_split(test_size=0.20)\n",
    "train1_dataset = train_test_split['train']\n",
    "remaining1_dataset = train_test_split['test']\n",
    "\n",
    "# Second split\n",
    "eval_test_split = remaining1_dataset.train_test_split(test_size=0.5)\n",
    "eval1_dataset = eval_test_split['train']\n",
    "test1_dataset = eval_test_split['test']\n",
    "\n",
    "# Combine datasets into a DatasetDict\n",
    "split_datasets = DatasetDict({\n",
    "    'train': train1_dataset,\n",
    "    'validation': eval1_dataset,\n",
    "    'test': test1_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "babdcb1447154093aee7a1dcd0687a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510b7d96ec594a22be5c1d8218124a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7f154108ac4fdbb22c06e25b5f69b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    # Remove '<s>' token\n",
    "    text = text.replace('<s>', '')\n",
    "    # Replace double spaces with single spaces\n",
    "    while \"  \" in text:\n",
    "        text = text.replace(\"  \", \" \")\n",
    "    return text.strip()  # Strip leading and trailing whitespace\n",
    "\n",
    "final_train1_dataset = train1_dataset.map(lambda item: {'title': clean_text(item['title']),\n",
    "                                                                    'summary': clean_text(item['summary'])})\n",
    "final_eval1_dataset = eval1_dataset.map(lambda item: {'title': clean_text(item['title']),\n",
    "                                                                  'summary': clean_text(item['summary'])})\n",
    "final_test1_dataset = test1_dataset.map(lambda item: {'title': clean_text(item['title']),\n",
    "                                                                  'summary': clean_text(item['summary'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the processed and segmented dataset from the saved JSON file\n",
    "dataset = load_dataset('json', data_files='/root/llama/datasets/small_segmented_subset_papers.json', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "# First split\n",
    "train_test_split = dataset.train_test_split(test_size=0.5)\n",
    "train_dataset = train_test_split['train']\n",
    "remaining_dataset = train_test_split['test']\n",
    "\n",
    "# Second split\n",
    "eval_test_split = remaining_dataset.train_test_split(test_size=0.5)\n",
    "eval_dataset = eval_test_split['train']\n",
    "test_dataset = eval_test_split['test']\n",
    "\n",
    "# Combine datasets into a DatasetDict\n",
    "split_datasets = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': eval_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_summary(example, tokenizer, max_length, overlap):\n",
    "    tokens = tokenizer.encode(example['summary'], add_special_tokens=True)\n",
    "    total_tokens = len(tokens)\n",
    "    start = 0\n",
    "    segments = []\n",
    "\n",
    "    while start < total_tokens:\n",
    "        end = start + max_length\n",
    "        segment = tokens[start:end]\n",
    "        segments.append(tokenizer.decode(segment))\n",
    "        start = end - overlap\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this was done once, when creating the segmented dataset, once we create the dataset, this step should be skipped\n",
    "\n",
    "def create_segmented_dataset(dataset, tokenizer, max_length, overlap):\n",
    "    segmented_dataset = []\n",
    "\n",
    "    for item in dataset:\n",
    "        summary_segments = sliding_window_summary(item, tokenizer, max_length, overlap)\n",
    "        for segment in summary_segments:\n",
    "            new_item = {'title': item['title'], 'summary_segment': segment}\n",
    "            segmented_dataset.append(new_item)\n",
    "\n",
    "    return segmented_dataset\n",
    "\n",
    "max_seq_length=200\n",
    "overlap=20\n",
    "\n",
    "# Apply it to the train, test, and eval datasets\n",
    "segmented_train_dataset = create_segmented_dataset(train_dataset, tokenizer, max_seq_length, overlap)\n",
    "segmented_test_dataset = create_segmented_dataset(test_dataset, tokenizer, max_seq_length, overlap)\n",
    "segmented_eval_dataset = create_segmented_dataset(eval_dataset, tokenizer, max_seq_length, overlap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "segmented_train_dataset = Dataset.from_pandas(pd.DataFrame(segmented_train_dataset))\n",
    "segmented_test_dataset = Dataset.from_pandas(pd.DataFrame(segmented_test_dataset))\n",
    "segmented_eval_dataset = Dataset.from_pandas(pd.DataFrame(segmented_eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove '<s>' token\n",
    "    text = text.replace('<s>', '')\n",
    "    # Replace double spaces with single spaces\n",
    "    while \"  \" in text:\n",
    "        text = text.replace(\"  \", \" \")\n",
    "    return text.strip()  # Strip leading and trailing whitespace\n",
    "\n",
    "segmented_train_dataset = segmented_train_dataset.map(lambda item: {'title': clean_text(item['title']),\n",
    "                                                                    'summary_segment': clean_text(item['summary_segment'])})\n",
    "segmented_eval_dataset = segmented_eval_dataset.map(lambda item: {'title': clean_text(item['title']),\n",
    "                                                                  'summary_segment': clean_text(item['summary_segment'])})\n",
    "segmented_test_dataset = segmented_test_dataset.map(lambda item: {'title': clean_text(item['title']),\n",
    "                                                                  'summary_segment': clean_text(item['summary_segment'])})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell is a demonstration of how we saved the processed and segmented dataset, should be skipped\n",
    "import pandas as pd\n",
    "#Convert the dataset to a pandas DataFrame\n",
    "train_df = pd.DataFrame(segmented_train_dataset)\n",
    "# Saving the DataFrame to a JSON file\n",
    "train_df.to_json('/root/llama/datasets/small_segmented_subset_papers.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_parameters = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir = \"./results\"\n",
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "optim = \"paged_adamw_32bit\"\n",
    "save_steps = 100\n",
    "logging_steps = 50\n",
    "learning_rate = 2e-4\n",
    "max_grad_norm = 0.4\n",
    "#max_steps = 100\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    evaluation_strategy=\"steps\", \n",
    "    eval_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=True,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    #max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f63dc3e6124ecfa075e18798a16d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/778 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b73e32d7be46c4af05f5c9c24b451c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/389 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 400\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset, \n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_parameters,\n",
    "    dataset_text_field=\"summary_segment\",  \n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_wandb = False\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='480' max='480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [480/480 12:21, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.095800</td>\n",
       "      <td>2.869321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.823200</td>\n",
       "      <td>2.899711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.536800</td>\n",
       "      <td>3.012022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.239200</td>\n",
       "      <td>3.190259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=480, training_loss=2.648106098175049, metrics={'train_runtime': 744.604, 'train_samples_per_second': 10.449, 'train_steps_per_second': 0.645, 'total_flos': 2.0005968033202176e+16, 'train_loss': 2.648106098175049, 'epoch': 9.85})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(\"outputs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig.from_pretrained('outputs')\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = \"What is the Big Bang theory? ### Assistant:\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=400)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/newpeft/lib/python3.9/site-packages/transformers/generation/utils.py:1468: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def condense_summary(summary):\n",
    "    # Remove headings and numbers\n",
    "    summary = re.sub(r'(\\n### .+\\n)|(\\d+\\.\\s)', '', summary)\n",
    "\n",
    "    # Split into sentences\n",
    "    sentences = re.split(r'\\.\\s+', summary)\n",
    "\n",
    "    # Remove duplicate sentences\n",
    "    seen = set()\n",
    "    unique_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if sentence not in seen:\n",
    "            unique_sentences.append(sentence)\n",
    "            seen.add(sentence)\n",
    "\n",
    "    # Join sentences into a paragraph\n",
    "    condensed_summary = '. '.join(unique_sentences)\n",
    "\n",
    "    return condensed_summary.strip()\n",
    "\n",
    "num_titles_to_generate = 10  # Set the number of titles you want to generate summaries for\n",
    "\n",
    "prompts = []\n",
    "for index, data in enumerate(train_dataset):\n",
    "    if index >= num_titles_to_generate:\n",
    "        break  \n",
    "    title = data['title']\n",
    "    prompt = f\"The following title is from a research paper in the fields of Machine Learning (ML), Artificial Intelligence (AI) and related subfields. Please provide a concise summary in an academic style that covers the main points and conclusions for the paper titled '{title}'. Directly start with the summary.\"\n",
    "    prompts.append(prompt)\n",
    "\n",
    "# Assuming you have a list 'actual_summaries' corresponding to these titles\n",
    "model_outputs = []\n",
    "for prompt in prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=400)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Post-processing to remove the echoed prompt\n",
    "    if generated_text.startswith(prompt):\n",
    "        generated_text = generated_text[len(prompt):].strip()\n",
    "\n",
    "    # Condense the generated text\n",
    "    condensed_text = condense_summary(generated_text)\n",
    "\n",
    "    model_outputs.append(condensed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/newpeft/lib/python3.9/site-packages/transformers/generation/utils.py:1468: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Here we created a title-to-summary mapping from the dataset with whole summaries, which we'll use for evaluation later\n",
    "title_to_summary_mapping = {data['title']: data['summary'] for data in final_train1_dataset}\n",
    "\n",
    "# Now we generate summaries based on titles from segmented dataset\n",
    "generated_summaries = []\n",
    "actual_summaries = []\n",
    "def condense_summary(summary):\n",
    "    # Removing headings and numbers\n",
    "    summary = re.sub(r'(\\n### .+\\n)|(\\d+\\.\\s)', '', summary)\n",
    "\n",
    "    # Splitting into sentences\n",
    "    sentences = re.split(r'\\.\\s+', summary)\n",
    "\n",
    "    # Removal of duplicate sentences\n",
    "    seen = set()\n",
    "    unique_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if sentence not in seen:\n",
    "            unique_sentences.append(sentence)\n",
    "            seen.add(sentence)\n",
    "\n",
    "    # Joining sentences into a paragraph\n",
    "    condensed_summary = '. '.join(unique_sentences)\n",
    "\n",
    "    return condensed_summary.strip()\n",
    "\n",
    "num_titles_to_generate = 20  \n",
    "prompts = []\n",
    "for index, data in enumerate(train_dataset):\n",
    "    if index >= num_titles_to_generate:\n",
    "        break  \n",
    "    title = data['title']\n",
    "    prompt = f\"The following title is from a research paper in the fields of Machine Learning (ML), Artificial Intelligence (AI) and related subfields. Please provide a concise summary in an academic style that covers the main points and conclusions for the paper titled '{title}'. Directly start with the summary.\"\n",
    "    prompts.append(prompt)\n",
    "\n",
    "# Using the 'actual_summaries' corresponding to these titles\n",
    "for prompt in prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=400)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Post-processing to remove the echoed prompt\n",
    "    if generated_text.startswith(prompt):\n",
    "        generated_text = generated_text[len(prompt):].strip()\n",
    "\n",
    "    condensed_text = condense_summary(generated_text)\n",
    "    generated_summaries.append(condensed_text)\n",
    "\n",
    "    # Fetch the actual summary from train1_dataset based on the title\n",
    "    actual_summary = title_to_summary_mapping.get(title, \"No matching summary found\")\n",
    "    actual_summaries.append(actual_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "The following title is from a research paper in the fields of Machine Learning (ML), Artificial Intelligence (AI) and related subfields. Please provide a concise summary in an academic style that covers the main points and conclusions for the paper titled 'automagically encoding adverse drug reactions in meddra'. Directly start with the summary.\n",
      "\n",
      "Generated Summary:\n",
      "Title: Automagically Encoding Adverse Drug Reactions in MedDRA\n",
      "\n",
      "Summary:\n",
      "\n",
      "In this paper, the authors propose a novel approach for automagically encoding adverse drug reactions (ADRs) in the Medical Dictionary for Regulatory Activities (MedDRA). The proposed method leverages the power of natural language processing (NLP) and machine learning (ML) techniques to automatically identify and encode ADRs in clinical trial reports. The authors evaluate the performance of their approach using a dataset of clinical trial reports and demonstrate that their method can accurately identify and encode ADRs with a high degree of precision. The authors also compare their approach with existing manual methods and show that their automated approach is more efficient and effective. The paper concludes by highlighting the potential benefits of using automagic encoding of ADRs in MedDRA, including improved data quality, increased efficiency, and reduced workload for regulatory professionals. Conclusion:\n",
      "\n",
      "The proposed approach has the potential to significantly improve the accuracy and efficiency of ADR encoding in MedDRA. By leveraging NLP and ML techniques, the authors are able to automatically identify and encode ADRs in clinical trial reports, which can lead to improved data quality, increased efficiency, and reduced workload for regulatory professionals. The proposed approach has important implications for the field of drug safety and regulatory affairs, and the authors suggest that it could be used as a valuable tool for regulatory professionals in the future.\n",
      "\n",
      "Actual Summary:\n",
      "in this paper we study the problem of author identification under doubleblind review setting which is to identify potential authors given information of an anonymized paper different from existing approaches that rely heavily on feature engineering we propose to use network embedding approach to address the problem which can automatically represent nodes into lower dimensional feature vectors however there are two major limitations in recent studies on network embedding 1 they are usually generalpurpose embedding methods which are independent of the specific tasks and 2 most of these approaches can only deal with homogeneous networks where the heterogeneity of the network is ignored hence challenges faced here are two folds 1 how to embed the network under the guidance of the author identification task and 2 how to select the best type of information due to the heterogeneity of the network to address the challenges we propose a taskguided and pathaugmented heterogeneous network embedding model in our model nodes are first embedded as vectors in latent feature space embeddings are then shared and jointly trained according to taskspecific and networkgeneral objectives we extend the existing unsupervised network embedding to incorporate meta paths in heterogeneous networks and select paths according to the specific task the guidance from author identification task for network embedding is provided both explicitly in joint training and implicitly during meta path selection our experiments demonstrate that by using pathaugmented network embedding with task guidance our model can obtain significantly better accuracy at identifying the true authors comparing to existing methods\n",
      "\n"
     ]
    }
   ],
   "source": [
    "specific_index = 0  # Lets test and view results for one title at a time\n",
    "if specific_index < len(prompts):\n",
    "    print(f\"Prompt:\\n{prompts[specific_index]}\\n\")\n",
    "    print(f\"Generated Summary:\\n{generated_summaries[specific_index]}\\n\")\n",
    "    print(f\"Actual Summary:\\n{actual_summaries[specific_index]}\\n\")\n",
    "else:\n",
    "    print(f\"No data available for index {specific_index}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.0179\n",
      "Average ROUGE-1 F1 Score: 0.2051\n",
      "Average ROUGE-2 F1 Score: 0.0232\n",
      "Average ROUGE-L F1 Score: 0.1584\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_bleu_scores(actual_summaries, generated_summaries):\n",
    "    # Calculating BLEU scores for each summary pair\n",
    "    smoothie = SmoothingFunction().method2\n",
    "    bleu_scores = [sentence_bleu([actual.split()], generated.split(), smoothing_function=smoothie) for actual, generated in zip(actual_summaries, generated_summaries)]\n",
    "    return bleu_scores\n",
    "\n",
    "def calculate_rouge_scores(actual_summaries, generated_summaries):\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = rouge.get_scores(generated_summaries, actual_summaries, avg=True)\n",
    "    return rouge_scores\n",
    "\n",
    "bleu_scores = calculate_bleu_scores(actual_summaries, generated_summaries)\n",
    "avg_bleu_score = np.mean(bleu_scores)\n",
    "rouge_scores = calculate_rouge_scores(actual_summaries, generated_summaries)\n",
    "\n",
    "# Extracting average F1 scores for each ROUGE metric\n",
    "rouge_1_f1 = rouge_scores['rouge-1']['f']\n",
    "rouge_2_f1 = rouge_scores['rouge-2']['f']\n",
    "rouge_l_f1 = rouge_scores['rouge-l']['f']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "metrics = ['BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
    "scores = [avg_bleu_score, rouge_1_f1, rouge_2_f1, rouge_l_f1]\n",
    "\n",
    "plt.bar(metrics, scores, color=['cyan', 'blue', 'green', 'red'])\n",
    "plt.title('Average BLEU and ROUGE Scores')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1) \n",
    "plt.show()\n",
    "\n",
    "# Printing BLEU and ROUGE scores\n",
    "print(f\"Average BLEU Score: {avg_bleu_score:.4f}\")\n",
    "print(f\"Average ROUGE-1 F1 Score: {rouge_1_f1:.4f}\")\n",
    "print(f\"Average ROUGE-2 F1 Score: {rouge_2_f1:.4f}\")\n",
    "print(f\"Average ROUGE-L F1 Score: {rouge_l_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newpeft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

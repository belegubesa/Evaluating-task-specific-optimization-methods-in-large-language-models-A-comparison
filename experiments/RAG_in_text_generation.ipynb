{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=embed_model_id,\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'device': device, 'batch_size': 32}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone\n",
    "\n",
    "#Pinecone initialization\n",
    "pinecone.init(\n",
    "    api_key=os.environ.get('PINECONE_API_KEY') or '27f3cdbb-b72d-4e4b-9351-e221b0e9deae',\n",
    "    environment=os.environ.get('PINECONE_ENVIRONMENT') or 'gcp-starter'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"document sample 1\",\n",
    "    \"document sample 2\"\n",
    "]\n",
    "\n",
    "embeddings = embed_model.embed_documents(docs)\n",
    "\n",
    "print(f\"We have {len(embeddings)} doc embeddings, each with \"\n",
    "      f\"a dimensionality of {len(embeddings[0])}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following line is used only in case the index is stuffed and we want to reiterate\n",
    "#pinecone.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = 'thesis-days'\n",
    "\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        index_name,\n",
    "        dimension=len(embeddings[0]),\n",
    "        metric='cosine'\n",
    "    )\n",
    "    # wait for index to finish initialization\n",
    "    while not pinecone.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pinecone.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('json', data_files='/root/llama/datasets/preprocessed_dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "# Step 1: Define a cleaning function\n",
    "def clean_text(example):\n",
    "    # Replace newline characters with space\n",
    "    example['title'] = example['title'].replace('\\n', ' ')\n",
    "    example['summary'] = example['summary'].replace('\\n', ' ')\n",
    "    # Replace apostrophes with empty string\n",
    "    example['title'] = example['title'].replace('\\'', '')\n",
    "    example['summary'] = example['summary'].replace('\\'', '')\n",
    "\n",
    "    # Add more cleaning steps as needed\n",
    "    return example\n",
    "\n",
    "# Step 3: Apply the cleaning function\n",
    "cleaned_dataset = dataset['train'].map(clean_text)\n",
    "\n",
    "# Step 4: Inspect a few records to ensure cleaning is done\n",
    "print(cleaned_dataset[0:2])\n",
    "\n",
    "# Save the cleaned dataset if needed\n",
    "#cleaned_dataset.save_to_disk('/path/to/save/dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = cleaned_dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = train_dataset.to_pandas()\n",
    "\n",
    "batch_size = 32\n",
    "id_offset = 0\n",
    "\n",
    "for i in range(0, len(dataset), batch_size):\n",
    "    i_end = min(len(dataset), i + batch_size)\n",
    "    batch = dataset.iloc[i:i_end]\n",
    "\n",
    "    # Because 'title' and 'summary' are the new column names\n",
    "    chunk_ids = [str(id_offset + i + 1) for i, _ in enumerate(batch.iterrows())]\n",
    "\n",
    "    title_texts = [x['title'] for _, x in batch.iterrows()]\n",
    "    summary_texts = [x['summary'] for _, x in batch.iterrows()]\n",
    "\n",
    "    # Embed_documents will still be used for the new format\n",
    "    title_embeds = embed_model.embed_documents(title_texts)\n",
    "    summary_embeds = embed_model.embed_documents(summary_texts)\n",
    "\n",
    "    metadata_chunks = [\n",
    "    {'text': \"This research paper, titled '\" + title + \"', falls within the scope of Artificial Intelligence (AI) and its related fields. Its main points and conclusions are presented in an academic style in the following abstract: '\" + summary + \"'.\"} \n",
    "    for title, summary in zip(title_texts, summary_texts)\n",
    "]\n",
    "    for chunk_id, title_embed, summary_embed, metadata_chunk in zip(chunk_ids, title_embeds, summary_embeds, metadata_chunks):\n",
    "        # Adjusted id to include chunk information, and encode to ASCII\n",
    "        adjusted_id = f\"{chunk_id}_{metadata_chunk['text']}\".encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "        # Truncated the adjusted_id to fit within the length limit\n",
    "        adjusted_id = adjusted_id[:512]\n",
    "\n",
    "        index.upsert(vectors=[(adjusted_id, title_embed, metadata_chunk)])\n",
    "\n",
    "    id_offset += len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first few metadata entries to check\n",
    "for i, metadata in enumerate(metadata_chunks[:5]):\n",
    "    print(f\"Metadata {i}: {metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "\n",
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "#model_id = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#set quantization \n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "hf_auth = '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'\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    #device_map='auto',\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    temperature=0.6, \n",
    "    max_new_tokens=400,  \n",
    "    repetition_penalty=1.0  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = generate_text(\"Can you explain to me the Big Bang Theory?\")\n",
    "print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(prompt=\"Can you explain to me the Big Bang Theory?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = 'text'  # field in metadata that contains text content\n",
    "\n",
    "vectorstore = Pinecone(\n",
    "    index, embed_model.embed_query, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "rag_pipeline = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type='stuff',\n",
    "    retriever=vectorstore.as_retriever()\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the title of the paper\n",
    "title = \"dual recurrent attention units for visual question answering\"\n",
    "\n",
    "# Create the prompt\n",
    "generated_text = f\"The following title is from a research paper in the fields of Machine Learning (ML), Artificial Intelligence (AI) and related subfields. Please provide a concise summary/abtract in an academic style for the paper titled '{title}'. Directly start with the summary.\"\n",
    "\n",
    "# Assuming you have a RAG pipeline set up as rag_pipeline\n",
    "response = rag_pipeline(generated_text)\n",
    "\n",
    "# Print the generated response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the generated responses\n",
    "generated_responses = []\n",
    "\n",
    "# Specify the number of titles to generate responses for (e.g., 5 titles)\n",
    "num_titles_to_generate = 20\n",
    "\n",
    "# Iterate over the dataset\n",
    "for index, data in enumerate(test_dataset):\n",
    "    if index >= num_titles_to_generate:\n",
    "        break  \n",
    "    title = data['title']\n",
    "    generated_text = f\"The following title is from a research paper in the fields of Machine Learning (ML), Artificial Intelligence (AI) and related subfields. Please provide a concise summary for the paper titled {title}. Directly start with the summary.\"\n",
    "    \n",
    "    # Use the RAG pipeline with the generated_text\n",
    "    response = rag_pipeline(generated_text)\n",
    "    \n",
    "    # Append the generated response (result) to the list\n",
    "    generated_responses.append(response['result'])  # Extract 'result' from the response dictionary\n",
    "\n",
    "# Now, generated_responses will contain only the results (generated summaries) for the first 5 titles in the dataset\n",
    "\n",
    "# Print one of the generated responses\n",
    "\n",
    "selected_response_1 = generated_responses[0]\n",
    "print(\"Response for the first title:\")\n",
    "print(selected_response_1)\n",
    "\n",
    "# Print the response for the second title (index 1)\n",
    "selected_response_2 = generated_responses[1]\n",
    "print(\"\\nResponse for the second title:\")\n",
    "print(selected_response_2)\n",
    "\n",
    "# The rest of your code for actual_summaries remains unchanged\n",
    "actual_titles = [test_dataset[i]['title'] for i in range(0, 20)]\n",
    "actual_summaries = [test_dataset[i]['summary'] for i in range(0, 20)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have already generated responses and extracted actual summaries as shown in the previous code\n",
    "\n",
    "# Print two examples of generated result and actual result\n",
    "for i in range(4):\n",
    "    print(f\"Title:\")\n",
    "    print(actual_titles[i])  # Print the actual summary\n",
    "    print(\"Generated Result:\")\n",
    "    print(generated_responses[i])  # Print the generated result (a string)\n",
    "    print(\"Actual Result:\")\n",
    "    print(actual_summaries[i])  # Print the actual summary\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_bleu_scores(actual_summaries, generated_responses):\n",
    "    # Calculate BLEU scores for each summary pair\n",
    "    # Ensure the summaries are split into words (tokens)\n",
    "    smoothie = SmoothingFunction().method2\n",
    "    bleu_scores = [sentence_bleu([actual.split()], generated.split(), smoothing_function=smoothie) for actual, generated in zip(actual_summaries, generated_responses)]\n",
    "    return bleu_scores\n",
    "\n",
    "def calculate_rouge_scores(actual_summaries, generated_responses):\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = rouge.get_scores(generated_responses, actual_summaries, avg=True)\n",
    "    return rouge_scores\n",
    "\n",
    "# Ensure that actual_summaries and generated_summaries are lists of strings\n",
    "# Calculate scores\n",
    "bleu_scores = calculate_bleu_scores(actual_summaries, generated_responses)\n",
    "avg_bleu_score = np.mean(bleu_scores)\n",
    "rouge_scores = calculate_rouge_scores(actual_summaries, generated_responses)\n",
    "\n",
    "# Extract average F1 scores for each ROUGE metric\n",
    "rouge_1_f1 = rouge_scores['rouge-1']['f']\n",
    "rouge_2_f1 = rouge_scores['rouge-2']['f']\n",
    "rouge_l_f1 = rouge_scores['rouge-l']['f']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "metrics = ['BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
    "scores = [avg_bleu_score, rouge_1_f1, rouge_2_f1, rouge_l_f1]\n",
    "\n",
    "plt.bar(metrics, scores, color=['cyan', 'blue', 'green', 'red'])\n",
    "plt.title('Average BLEU and ROUGE Scores')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)  # Assuming the scores are normalized between 0 and 1\n",
    "plt.show()\n",
    "\n",
    "# Printing BLEU and ROUGE scores\n",
    "print(f\"Average BLEU Score: {avg_bleu_score:.4f}\")\n",
    "print(f\"Average ROUGE-1 F1 Score: {rouge_1_f1:.4f}\")\n",
    "print(f\"Average ROUGE-2 F1 Score: {rouge_2_f1:.4f}\")\n",
    "print(f\"Average ROUGE-L F1 Score: {rouge_l_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewEnvPy39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

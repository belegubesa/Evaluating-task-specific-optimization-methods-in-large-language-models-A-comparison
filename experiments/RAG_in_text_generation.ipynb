{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/NewEnvPy39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/NewEnvPy39/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=embed_model_id,\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'device': device, 'batch_size': 32}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone\n",
    "\n",
    "#Pinecone initialization\n",
    "pinecone.init(\n",
    "    api_key=os.environ.get('PINECONE_API_KEY') or '27f3cdbb-b72d-4e4b-9351-e221b0e9deae',\n",
    "    environment=os.environ.get('PINECONE_ENVIRONMENT') or 'gcp-starter'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 2 doc embeddings, each with a dimensionality of 384.\n"
     ]
    }
   ],
   "source": [
    "docs = [\n",
    "    \"document sample 1\",\n",
    "    \"document sample 2\"\n",
    "]\n",
    "\n",
    "embeddings = embed_model.embed_documents(docs)\n",
    "\n",
    "print(f\"We have {len(embeddings)} doc embeddings, each with \"\n",
    "      f\"a dimensionality of {len(embeddings[0])}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following line is used only in case the index is stuffed and we want to reiterate\n",
    "#pinecone.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = 'thesis-days'\n",
    "\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        index_name,\n",
    "        dimension=len(embeddings[0]),\n",
    "        metric='cosine'\n",
    "    )\n",
    "    # wait for index to finish initialization\n",
    "    while not pinecone.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pinecone.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('json', data_files='/root/llama/datasets/preprocessed_dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summary': ['we propose an architecture for vqa which utilizes recurrent layers to generate visual and textual attention the memory characteristic of the proposed recurrent attention units offers a rich joint embedding of visual and textual features and enables the model to reason relations between several parts of the image and question our single model outperforms the first place winner on the vqa 10 dataset performs within margin to the current stateoftheart ensemble model we also experiment with replacing attention mechanisms in other stateoftheart models with our implementation and show increased accuracy in both cases our recurrent attention mechanism improves performance in tasks requiring sequential or relational reasoning on the vqa dataset', 'recent approaches based on artificial neural networks anns have shown promising results for shorttext classification however many short texts occur in sequences eg sentences in a document or utterances in a dialog and most existing annbased systems do not leverage the preceding short texts when classifying a subsequent one in this work we present a model based on recurrent neural networks and convolutional neural networks that incorporates the preceding short texts our model achieves stateoftheart results on three different datasets for dialog act prediction'], 'title': ['dual recurrent attention units for visual question answering', 'sequential shorttext classification with recurrent and convolutional   neural networks']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "# Step 1: Define a cleaning function\n",
    "def clean_text(example):\n",
    "    # Replace newline characters with space\n",
    "    example['title'] = example['title'].replace('\\n', ' ')\n",
    "    example['summary'] = example['summary'].replace('\\n', ' ')\n",
    "    # Replace apostrophes with empty string\n",
    "    example['title'] = example['title'].replace('\\'', '')\n",
    "    example['summary'] = example['summary'].replace('\\'', '')\n",
    "\n",
    "    # Add more cleaning steps as needed\n",
    "    return example\n",
    "\n",
    "# Step 3: Apply the cleaning function\n",
    "cleaned_dataset = dataset['train'].map(clean_text)\n",
    "\n",
    "# Step 4: Inspect a few records to ensure cleaning is done\n",
    "print(cleaned_dataset[0:2])\n",
    "\n",
    "# Save the cleaned dataset if needed\n",
    "#cleaned_dataset.save_to_disk('/path/to/save/dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = cleaned_dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = train_dataset.to_pandas()\n",
    "\n",
    "batch_size = 32\n",
    "id_offset = 0\n",
    "\n",
    "for i in range(0, len(dataset), batch_size):\n",
    "    i_end = min(len(dataset), i + batch_size)\n",
    "    batch = dataset.iloc[i:i_end]\n",
    "\n",
    "    # Because 'title' and 'summary' are the new column names\n",
    "    chunk_ids = [str(id_offset + i + 1) for i, _ in enumerate(batch.iterrows())]\n",
    "\n",
    "    title_texts = [x['title'] for _, x in batch.iterrows()]\n",
    "    summary_texts = [x['summary'] for _, x in batch.iterrows()]\n",
    "\n",
    "    # Embed_documents will still be used for the new format\n",
    "    title_embeds = embed_model.embed_documents(title_texts)\n",
    "    summary_embeds = embed_model.embed_documents(summary_texts)\n",
    "\n",
    "    metadata_chunks = [\n",
    "    {'text': \"This research paper, titled '\" + title + \"', falls within the scope of Artificial Intelligence (AI) and its related fields. Its main points and conclusions are presented in an academic style in the following abstract: '\" + summary + \"'.\"} \n",
    "    for title, summary in zip(title_texts, summary_texts)\n",
    "]\n",
    "    for chunk_id, title_embed, summary_embed, metadata_chunk in zip(chunk_ids, title_embeds, summary_embeds, metadata_chunks):\n",
    "        # Adjusted id to include chunk information, and encode to ASCII\n",
    "        adjusted_id = f\"{chunk_id}_{metadata_chunk['text']}\".encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "        # Truncated the adjusted_id to fit within the length limit\n",
    "        adjusted_id = adjusted_id[:512]\n",
    "\n",
    "        index.upsert(vectors=[(adjusted_id, title_embed, metadata_chunk)])\n",
    "\n",
    "    id_offset += len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata 0: {'text': \"This research paper, titled 'sharing deep generative representation for perceived image   reconstruction from human brain activity', falls within the scope of Artificial Intelligence (AI) and its related fields. Its main points and conclusions are presented in an academic style in the following abstract: 'decoding human brain activities via functional magnetic resonance imaging fmri has gained increasing attention in recent years while encouraging results have been reported in brain states classification tasks reconstructing the details of human visual experience still remains difficult two main challenges that hinder the development of effective models are the perplexing fmri measurement noise and the high dimensionality of limited data instances existing methods generally suffer from one or both of these issues and yield dissatisfactory results in this paper we tackle this problem by casting the reconstruction of visual stimulus as the bayesian inference of missing view in a multiview latent variable model sharing a common latent representation our joint generative model of external stimulus and brain response is not only deep in extracting nonlinear features from visual images but also powerful in capturing correlations among voxel activities of fmri recordings the nonlinearity and deep structure endow our model with strong representation ability while the correlations of voxel activities are critical for suppressing noise and improving prediction we devise an efficient variational bayesian method to infer the latent variables and the model parameters to further improve the reconstruction accuracy the latent representations of testing instances are enforced to be close to that of their neighbours from the training set via posterior regularization experiments on three fmri recording datasets demonstrate that our approach can more accurately reconstruct visual stimuli'.\"}\n",
      "Metadata 1: {'text': \"This research paper, titled 'multispectral compressive imaging strategies using fabrypérot   filtered sensors', falls within the scope of Artificial Intelligence (AI) and its related fields. Its main points and conclusions are presented in an academic style in the following abstract: 'this paper introduces two acquisition device architectures for multispectral compressive imaging unlike most existing methods the proposed computational imaging techniques do not include any dispersive element as they use a dedicated sensor which integrates narrowband fabryperot spectral filters at the pixel level the first scheme leverages joint inpainting and superresolution to fill in those voxels that are missing due to the devices limited pixel count the second scheme in link with compressed sensing introduces spatial random convolutions but is more complex and may be affected by diffraction in both cases we solve the associated inverse problems by using the same signal prior specifically we propose a redundant analysis signal prior in a convex formulation through numerical simulations we explore different realistic setups our objective is also to highlight some practical guidelines and discuss their complexity tradeoffs to integrate these schemes into actual computational imaging systems our conclusion is that the second technique performs best at high compression levels in a properly sized and calibrated setup otherwise the first simpler technique should be favored'.\"}\n",
      "Metadata 2: {'text': \"This research paper, titled 'random forests some methodological insights', falls within the scope of Artificial Intelligence (AI) and its related fields. Its main points and conclusions are presented in an academic style in the following abstract: 'this paper examines from an experimental perspective random forests the increasingly used statistical method for classification and regression problems introduced by leo breiman in 2001 it first aims at confirming known but sparse advice for using random forests and at proposing some complementary remarks for both standard problems as well as high dimensional ones for which the number of variables hugely exceeds the sample size but the main contribution of this paper is twofold to provide some insights about the behavior of the variable importance index based on random forests and in addition to propose to investigate two classical issues of variable selection the first one is to find important variables for interpretation and the second one is more restrictive and try to design a good prediction model the strategy involves a ranking of explanatory variables using the random forests score of importance and a stepwise ascending variable introduction strategy'.\"}\n",
      "Metadata 3: {'text': \"This research paper, titled 'unsupervised ensemble regression', falls within the scope of Artificial Intelligence (AI) and its related fields. Its main points and conclusions are presented in an academic style in the following abstract: 'consider a regression problem where there is no labeled data and the only observations are the predictions fixj of m experts fi over many samples xj with no knowledge on the accuracy of the experts is it still possible to accurately estimate the unknown responses yj can one still detect the least or most accurate experts in this work we propose a framework to study these questions based on the assumption that the m experts have uncorrelated deviations from the optimal predictor assuming the first two moments of the response are known we develop methods to detect the best and worst regressors and derive upcr a novel principal components approach for unsupervised ensemble regression we provide theoretical support for upcr and illustrate its improved accuracy over the ensemble mean and median on a variety of regression problems'.\"}\n"
     ]
    }
   ],
   "source": [
    "# Print first few metadata entries to check\n",
    "for i, metadata in enumerate(metadata_chunks[:5]):\n",
    "    print(f\"Metadata {i}: {metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.041,\n",
       " 'namespaces': {'': {'vector_count': 4100}},\n",
       " 'total_vector_count': 4100}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "\n",
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "#model_id = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#set quantization \n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "hf_auth = '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'\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    #device_map='auto',\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    temperature=0.6, \n",
    "    max_new_tokens=400,  \n",
    "    repetition_penalty=1.0  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you explain to me the Big Bang Theory?\n",
      " nobody knows what happened before the Big Bang, but scientists have developed several theories to explain what might have caused the universe to come into existence.\n",
      "The Big Bang theory is the leading explanation for the origins of the universe, based on a large body of observational evidence from many fields of science. According to the theory, the universe began as an infinitely hot and dense point around 13.8 billion years ago.\n",
      "The Big Bang theory is the leading explanation for the origins of the universe, based on a large body of observational evidence from many fields of science. According to the theory, the universe began as an infinitely hot and dense point around 13.8 billion years ago. The singularity expanded rapidly, and as it did, it cooled and formed the particles, atoms, and eventually the stars and galaxies we see today.\n",
      "The Big Bang theory is supported by a wide range of observations, including the cosmic microwave background radiation, the abundance of light elements, and the large-scale structure of the universe. The cosmic microwave background radiation is thought to be the residual heat from the initial explosion, and it is observed to be uniform throughout the universe, with tiny fluctuations that are thought to have given rise to the structures we see today.\n",
      "The abundance of light elements, such as hydrogen and helium, is also consistent with the predictions of the Big Bang theory, as these elements are thought to have been formed in the first few minutes after the universe began. Finally, the large-scale structure of the universe, including the distribution of galaxies and galaxy clusters, is also consistent with the predictions of the Big Bang theory.\n",
      "While the Big Bang theory is the leading explanation for the origins of the universe, there are still many unanswered questions about the details of the early universe. For example, scientists still do not know what caused the Big Bang,\n"
     ]
    }
   ],
   "source": [
    "res = generate_text(\"Can you explain to me the Big Bang Theory?\")\n",
    "print(res[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n nobody likes to be around negative people, so it's important to surround yourself with positive influences.\\nThe Big Bang Theory is a scientific theory that explains the origins of the universe. According to the theory, the universe began as a single point, known as a singularity, around 13.8 billion years ago. This singularity contained all of the matter and energy that would eventually make up the universe, packed into a space smaller than an atomic nucleus.\\nAt some point, this singularity suddenly expanded, causing the universe to rapidly inflate and cool. This inflationary period, known as the Big Bang, is thought to have lasted for a fraction of a second, but it was long enough to create the conditions for the formation of the first subatomic particles, such as protons, neutrons, and electrons. These particles then combined to form atoms, which eventually coalesced into the first stars and galaxies.\\nOver time, these stars and galaxies coalesced into the large-scale structures we see in the universe today, such as galaxy clusters and superclusters. The universe has been expanding and cooling ever since the Big Bang, and it continues to do so today.\\nThe Big Bang Theory is supported by a wide range of observational evidence, including the cosmic microwave background radiation, the abundance of light elements, and the large-scale structure of the universe. The cosmic microwave background radiation is thought to be the residual heat from the early universe, and it is observed to be uniform throughout the universe, with tiny fluctuations that are thought to have seeded the formation of galaxies. The abundance of light elements, such as hydrogen and helium, is also consistent with the predictions of the Big Bang Theory, as these elements are thought to have been formed in the first few minutes after the universe began. Finally, the large-scale\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(prompt=\"Can you explain to me the Big Bang Theory?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = 'text'  # field in metadata that contains text content\n",
    "\n",
    "vectorstore = Pinecone(\n",
    "    index, embed_model.embed_query, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "rag_pipeline = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type='stuff',\n",
    "    retriever=vectorstore.as_retriever()\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': \"The following title is from a research paper in the fields of Machine Learning (ML), Artificial Intelligence (AI) and related subfields. Please provide a concise summary/abtract in an academic style for the paper titled 'dual recurrent attention units for visual question answering'. Directly start with the summary.\", 'result': \" The paper titled 'dual recurrent attention units for visual question answering' presents a novel approach to visual question answering (VQA) that leverages dual recurrent attention units (DRAU) to improve the model's ability to answer questions about complex visual scenes. The authors propose a new architecture that combines DRAU with a convolutional neural network (CNN) to form a dual-modal attention network (DMN). The DMN is trained on the VQA dataset and achieves state-of-the-art performance on the test set, outperforming previous state-of-the-art models. The authors also analyze the attention weights generated by the DRAU and find that they are effective in identifying the most relevant regions of the image for answering questions.\"}\n"
     ]
    }
   ],
   "source": [
    "# Define the title of the paper\n",
    "title = \"dual recurrent attention units for visual question answering\"\n",
    "\n",
    "# Create the prompt\n",
    "generated_text = f\"The following title is from a research paper in the fields of Machine Learning (ML), Artificial Intelligence (AI) and related subfields. Please provide a concise summary/abtract in an academic style for the paper titled '{title}'. Directly start with the summary.\"\n",
    "\n",
    "# Assuming you have a RAG pipeline set up as rag_pipeline\n",
    "response = rag_pipeline(generated_text)\n",
    "\n",
    "# Print the generated response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/NewEnvPy39/lib/python3.9/site-packages/transformers/pipelines/base.py:1083: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response for the first title:\n",
      " The paper proposes a new mechanism for crowdsourcing tasks, called Quizz, which combines elements of multiarmed bandit (MAB) and crowdsourcing. Quizz aims to identify knowledgeable users on the internet and incentivize them to complete tasks while ensuring the quality of the output. The mechanism uses a feedback-based approach to identify high-quality users and adjusts the reward distribution accordingly. The authors evaluate the effectiveness of Quizz through experiments involving over 10,000 users and show that it outperforms existing methods in terms of both efficiency and quality.\n",
      "\n",
      "Response for the second title:\n",
      " The paper proposes a novel approach for discovering topics in text datasets by visualizing relevant words. The proposed method leverages the concept of layerwise relevance propagation (LRP) to decompose the predictions of two word-based machine learning models into individual words. The scores obtained from LRP are used to generate vector-based document representations that capture semantic information. The authors demonstrate the effectiveness of their approach by showing that the resulting document vectors capture relevant information and provide a higher level of explainability compared to the original models.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the generated responses\n",
    "generated_responses = []\n",
    "\n",
    "# Specify the number of titles to generate responses for (e.g., 5 titles)\n",
    "num_titles_to_generate = 20\n",
    "\n",
    "# Iterate over the dataset\n",
    "for index, data in enumerate(test_dataset):\n",
    "    if index >= num_titles_to_generate:\n",
    "        break  \n",
    "    title = data['title']\n",
    "    generated_text = f\"The following title is from a research paper in the fields of Machine Learning (ML), Artificial Intelligence (AI) and related subfields. Please provide a concise summary for the paper titled {title}. Directly start with the summary.\"\n",
    "    \n",
    "    # Use the RAG pipeline with the generated_text\n",
    "    response = rag_pipeline(generated_text)\n",
    "    \n",
    "    # Append the generated response (result) to the list\n",
    "    generated_responses.append(response['result'])  # Extract 'result' from the response dictionary\n",
    "\n",
    "# Now, generated_responses will contain only the results (generated summaries) for the first 5 titles in the dataset\n",
    "\n",
    "# Print one of the generated responses\n",
    "\n",
    "selected_response_1 = generated_responses[0]\n",
    "print(\"Response for the first title:\")\n",
    "print(selected_response_1)\n",
    "\n",
    "# Print the response for the second title (index 1)\n",
    "selected_response_2 = generated_responses[1]\n",
    "print(\"\\nResponse for the second title:\")\n",
    "print(selected_response_2)\n",
    "\n",
    "# The rest of your code for actual_summaries remains unchanged\n",
    "actual_titles = [test_dataset[i]['title'] for i in range(0, 20)]\n",
    "actual_summaries = [test_dataset[i]['summary'] for i in range(0, 20)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:\n",
      "an incentive compatible multiarmedbandit crowdsourcing mechanism with   quality assurance\n",
      "Generated Result:\n",
      " The paper proposes a new mechanism for crowdsourcing tasks, called Quizz, which combines elements of multiarmed bandit (MAB) and crowdsourcing. Quizz aims to identify knowledgeable users on the internet and incentivize them to complete tasks while ensuring the quality of the output. The mechanism uses a feedback-based approach to identify high-quality users and adjusts the reward distribution accordingly. The authors evaluate the effectiveness of Quizz through experiments involving over 10,000 users and show that it outperforms existing methods in terms of both efficiency and quality.\n",
      "Actual Result:\n",
      "consider a requester who wishes to crowdsource a series of identical binary labeling tasks to a pool of workers so as to achieve an assured accuracy for each task in a cost optimal way the workers are heterogeneous with unknown but fixed qualities and their costs are private the problem is to select for each task an optimal subset of workers so that the outcome obtained from the selected workers guarantees a target accuracy level the problem is a challenging one even in a non strategic setting since the accuracy of aggregated label depends on unknown qualities we develop a novel multiarmed bandit mab mechanism for solving this problem first we propose a framework assured accuracy bandit aab which leads to an mab algorithm constrained confidence bound for a non strategic setting ccbns we derive an upper bound on the number of time steps the algorithm chooses a suboptimal set that depends on the target accuracy level and true qualities a more challenging situation arises when the requester not only has to learn the qualities of the workers but also elicit their true costs we modify the ccbns algorithm to obtain an adaptive exploration separated algorithm which we call  em constrained confidence bound for a strategic setting ccbs ccbs algorithm produces an expost monotone allocation rule and thus can be transformed into an expost incentive compatible and expost individually rational mechanism that learns the qualities of the workers and guarantees a given target accuracy level in a cost optimal way we provide a lower bound on the number of times any algorithm should select a suboptimal set and we see that the lower bound matches our upper bound upto a constant factor we provide insights on the practical implementation of this framework through an illustrative example and we show the efficacy of our algorithms through simulations\n",
      "\n",
      "\n",
      "Title:\n",
      "discovering topics in text datasets by visualizing relevant words\n",
      "Generated Result:\n",
      " The paper proposes a novel approach for discovering topics in text datasets by visualizing relevant words. The proposed method leverages the concept of layerwise relevance propagation (LRP) to decompose the predictions of two word-based machine learning models into individual words. The scores obtained from LRP are used to generate vector-based document representations that capture semantic information. The authors demonstrate the effectiveness of their approach by showing that the resulting document vectors capture relevant information and provide a higher level of explainability compared to the original models.\n",
      "Actual Result:\n",
      "when dealing with large collections of documents it is imperative to quickly get an overview of the texts contents in this paper we show how this can be achieved by using a clustering algorithm to identify topics in the dataset and then selecting and visualizing relevant words which distinguish a group of documents from the rest of the texts to summarize the contents of the documents belonging to each topic we demonstrate our approach by discovering trending topics in a collection of new york times article snippets\n",
      "\n",
      "\n",
      "Title:\n",
      "the delivermath project  text analysis in mathematics\n",
      "Generated Result:\n",
      " The DeliverMath project aims to develop methods for text analysis in mathematics, with a focus on automating the process of extracting mathematical concepts from text. The paper describes the use of machine learning techniques, including word embeddings and topic modeling, to analyze mathematical texts and identify relevant concepts. The authors also propose a new evaluation measure for assessing the quality of mathematical text analysis systems, based on the ability to identify and extract relevant concepts from a given text. Overall, the paper demonstrates the potential of machine learning techniques for improving the efficiency and accuracy of mathematical text analysis.\n",
      "Actual Result:\n",
      "a highquality content analysis is essential for retrieval functionalities but the manual extraction of key phrases and classification is expensive natural language processing provides a framework to automatize the process here a machinebased approach for the content analysis of mathematical texts is described a prototype for key phrase extraction and classification of mathematical texts is presented\n",
      "\n",
      "\n",
      "Title:\n",
      "depth estimation through a generative model of light field synthesis\n",
      "Generated Result:\n",
      " The research paper titled \"Depth Estimation through a Generative Model of Light Field Synthesis\" presents a novel approach to depth estimation using a generative model. The proposed method leverages the ability of Generative Adversarial Networks (GANs) to synthesize realistic light fields and uses the resulting images to estimate depth. The key idea is to use the GAN to generate a set of light fields that are then used to train a depth estimation network. The proposed method outperforms traditional depth estimation methods in terms of accuracy and computational efficiency.\n",
      "Actual Result:\n",
      "light field photography captures rich structural information that may facilitate a number of traditional image processing and computer vision tasks a crucial ingredient in such endeavors is accurate depth recovery we present a novel framework that allows the recovery of a high quality continuous depth map from light field data to this end we propose a generative model of a light field that is fully parametrized by its corresponding depth map the model allows for the integration of powerful regularization techniques such as a nonlocal means prior facilitating accurate depth map estimation\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have already generated responses and extracted actual summaries as shown in the previous code\n",
    "\n",
    "# Print two examples of generated result and actual result\n",
    "for i in range(4):\n",
    "    print(f\"Title:\")\n",
    "    print(actual_titles[i])  # Print the actual summary\n",
    "    print(\"Generated Result:\")\n",
    "    print(generated_responses[i])  # Print the generated result (a string)\n",
    "    print(\"Actual Result:\")\n",
    "    print(actual_summaries[i])  # Print the actual summary\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.0242\n",
      "Average ROUGE-1 F1 Score: 0.2206\n",
      "Average ROUGE-2 F1 Score: 0.0327\n",
      "Average ROUGE-L F1 Score: 0.1700\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_bleu_scores(actual_summaries, generated_responses):\n",
    "    # Calculate BLEU scores for each summary pair\n",
    "    # Ensure the summaries are split into words (tokens)\n",
    "    smoothie = SmoothingFunction().method2\n",
    "    bleu_scores = [sentence_bleu([actual.split()], generated.split(), smoothing_function=smoothie) for actual, generated in zip(actual_summaries, generated_responses)]\n",
    "    return bleu_scores\n",
    "\n",
    "def calculate_rouge_scores(actual_summaries, generated_responses):\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = rouge.get_scores(generated_responses, actual_summaries, avg=True)\n",
    "    return rouge_scores\n",
    "\n",
    "# Ensure that actual_summaries and generated_summaries are lists of strings\n",
    "# Calculate scores\n",
    "bleu_scores = calculate_bleu_scores(actual_summaries, generated_responses)\n",
    "avg_bleu_score = np.mean(bleu_scores)\n",
    "rouge_scores = calculate_rouge_scores(actual_summaries, generated_responses)\n",
    "\n",
    "# Extract average F1 scores for each ROUGE metric\n",
    "rouge_1_f1 = rouge_scores['rouge-1']['f']\n",
    "rouge_2_f1 = rouge_scores['rouge-2']['f']\n",
    "rouge_l_f1 = rouge_scores['rouge-l']['f']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "metrics = ['BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
    "scores = [avg_bleu_score, rouge_1_f1, rouge_2_f1, rouge_l_f1]\n",
    "\n",
    "plt.bar(metrics, scores, color=['cyan', 'blue', 'green', 'red'])\n",
    "plt.title('Average BLEU and ROUGE Scores')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)  # Assuming the scores are normalized between 0 and 1\n",
    "plt.show()\n",
    "\n",
    "# Printing BLEU and ROUGE scores\n",
    "print(f\"Average BLEU Score: {avg_bleu_score:.4f}\")\n",
    "print(f\"Average ROUGE-1 F1 Score: {rouge_1_f1:.4f}\")\n",
    "print(f\"Average ROUGE-2 F1 Score: {rouge_2_f1:.4f}\")\n",
    "print(f\"Average ROUGE-L F1 Score: {rouge_l_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewEnvPy39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

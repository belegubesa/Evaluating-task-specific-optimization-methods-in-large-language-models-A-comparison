{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "df = pd.read_json('/root/llama/datasets/preprocessed_dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert the pandas DataFrame to a HuggingFace Dataset\n",
    "hf_dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Step 1: Define a cleaning function\n",
    "def clean_text(example):\n",
    "    # Replace newline characters with space\n",
    "    example['title'] = example['title'].replace('\\n', ' ')\n",
    "    example['summary'] = example['summary'].replace('\\n', ' ')\n",
    "    # Replace apostrophes with empty string\n",
    "    example['title'] = example['title'].replace('\\'', '')\n",
    "    example['summary'] = example['summary'].replace('\\'', '')\n",
    "\n",
    "    # Add more cleaning steps as needed\n",
    "    return example\n",
    "\n",
    "# Step 3: Apply the cleaning function\n",
    "cleaned_dataset = hf_dataset.map(clean_text)\n",
    "\n",
    "# Step 4: Inspect a few records to ensure cleaning is done\n",
    "print(cleaned_dataset[0:2])\n",
    "\n",
    "# Save the cleaned dataset if needed\n",
    "#cleaned_dataset.save_to_disk('/path/to/save/dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaned_dataset[0])  # Print the first entry to check its structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#zero shot learning\n",
    "#prompts = [\n",
    "#    f\"The following is a research paper in the fields of Machine Learning, Artificial Intelligence and other related subfields. Please provide a concise summary that covers its main points and conclusions for the paper titled \\\"{cleaned_dataset[i]['title']}\\\".\"\n",
    "#    for i in range(50)\n",
    "#]\n",
    "\n",
    "#few shot learning\n",
    "#prompts = [\n",
    "#    f'Given a research paper title, provide a detailed summary that covers the main objectives, methods, findings, and conclusions.\\n'\n",
    "#    f'For example:\\n'\n",
    "#    f'For the title \"Dual Recurrent Attention Units for Visual Question Answering\", '\n",
    "#    f'the summary introduces an innovative architecture with dual recurrent attention units, discussing its mechanisms for enhancing visual and textual analysis and its general performance improvements.\\n\\n'\n",
    "#    f'Now, provide a summary for: \"{cleaned_dataset[i][\"title\"]}\"'\n",
    "#    for i in range(1, min(51, len(cleaned_dataset)))\n",
    "#]\n",
    "\n",
    "# chain-of-thoughts\n",
    "prompts = [\n",
    "    f'Given a research paper title, \"{cleaned_dataset[i][\"title\"]}\", provide a detailed summary:\\n'\n",
    "    f'1. Identify the main objectives of the research.\\n'\n",
    "    f'2. Describe the methods used to achieve these objectives.\\n'\n",
    "    f'3. Summarize the key findings and conclusions.\\n'\n",
    "    f'Your summary should be a single, cohesive paragraph without section breaks, where objectives, methods, and conclusions are not stated obviously but explained naturally.'\n",
    "    for i in range(0, min(50, len(cleaned_dataset)))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#postprocessing and cleaning the summary for more accurate evaluation\n",
    "import re\n",
    "def condense_summary(summary):\n",
    "    # Remove headings and numbers\n",
    "    summary = re.sub(r'(\\n### .+\\n)|(\\d+\\.\\s)', '', summary)\n",
    "\n",
    "    # Split into sentences\n",
    "    sentences = re.split(r'\\.\\s+', summary)\n",
    "\n",
    "    # Remove duplicate sentences\n",
    "    seen = set()\n",
    "    unique_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if sentence not in seen:\n",
    "            unique_sentences.append(sentence)\n",
    "            seen.add(sentence)\n",
    "\n",
    "    # Join sentences into a paragraph\n",
    "    condensed_summary = '. '.join(unique_sentences)\n",
    "\n",
    "    return condensed_summary.strip()\n",
    "\n",
    "model_outputs = []\n",
    "for prompt in prompts:\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    output_ids = model.generate(input_ids, max_length=250, temperature=0.4, top_p=0.6)\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Post-processing to remove the echoed prompt\n",
    "    if generated_text.startswith(prompt):\n",
    "        generated_text = generated_text[len(prompt):].strip()\n",
    "\n",
    "    # Condense the generated text\n",
    "    condensed_text = condense_summary(generated_text)\n",
    "\n",
    "    model_outputs.append(condensed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract generated plus actual summaries\n",
    "generated_summaries = model_outputs\n",
    "actual_summaries = [cleaned_dataset[i]['summary'] for i in range(50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing and comparing the first 2 model-generated summaries with the actual summaries\n",
    "for i in range(2):\n",
    "    print(f\"Prompt:\\n{prompts[i]}\\n\")\n",
    "    print(f\"Generated Summary:\\n{generated_summaries[i]}\\n\")\n",
    "    print(f\"Actual Summary:\\n{actual_summaries[i]}\\n\")\n",
    "    print(\"---------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the index you want to focus on\n",
    "index = 2  # Replace 0 with the desired index\n",
    "\n",
    "# Printing and comparing the model-generated summary with the actual summary for the specified index\n",
    "print(f\"Prompt:\\n{prompts[index]}\\n\")\n",
    "print(f\"Generated Summary:\\n{generated_summaries[index]}\\n\")\n",
    "print(f\"Actual Summary:\\n{actual_summaries[index]}\\n\")\n",
    "print(\"---------------------------------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_bleu_scores(actual_summaries, generated_summaries):\n",
    "    # Calculate BLEU scores for each summary pair\n",
    "    # Ensure the summaries are split into words (tokens)\n",
    "    smoothie = SmoothingFunction().method2\n",
    "    bleu_scores = [sentence_bleu([actual.split()], generated.split(), smoothing_function=smoothie) for actual, generated in zip(actual_summaries, generated_summaries)]\n",
    "    return bleu_scores\n",
    "\n",
    "def calculate_rouge_scores(actual_summaries, generated_summaries):\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = rouge.get_scores(generated_summaries, actual_summaries, avg=True)\n",
    "    return rouge_scores\n",
    "\n",
    "# Ensure that actual_summaries and generated_summaries are lists of strings\n",
    "# Calculate scores\n",
    "bleu_scores = calculate_bleu_scores(actual_summaries, generated_summaries)\n",
    "avg_bleu_score = np.mean(bleu_scores)\n",
    "rouge_scores = calculate_rouge_scores(actual_summaries, generated_summaries)\n",
    "\n",
    "# Extract average F1 scores for each ROUGE metric\n",
    "rouge_1_f1 = rouge_scores['rouge-1']['f']\n",
    "rouge_2_f1 = rouge_scores['rouge-2']['f']\n",
    "rouge_l_f1 = rouge_scores['rouge-l']['f']\n",
    "\n",
    "# Plotting and printing the BLEU and ROUGE scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "metrics = ['BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
    "scores = [avg_bleu_score, rouge_1_f1, rouge_2_f1, rouge_l_f1]\n",
    "\n",
    "plt.bar(metrics, scores, color=['cyan', 'blue', 'green', 'red'])\n",
    "plt.title('Average BLEU and ROUGE Scores')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average BLEU Score: {avg_bleu_score:.4f}\")\n",
    "print(f\"Average ROUGE-1 F1 Score: {rouge_1_f1:.4f}\")\n",
    "print(f\"Average ROUGE-2 F1 Score: {rouge_2_f1:.4f}\")\n",
    "print(f\"Average ROUGE-L F1 Score: {rouge_l_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

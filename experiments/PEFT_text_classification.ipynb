{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b673eec8589744ceb809b7276889f8db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: cardiovascular diseases\n",
      "Text: Catheterization laboratory events and hospital outcome with direct angioplasty for acute myocardial infarction To assess the safety of direct infarct angioplasty without antecedent thrombolytic therapy, catheterization laboratory and hospital events were assessed in consecutively treated patients with infarctions involving the left anterior descending (n = 100 patients), right (n = 100), and circumflex (n = 50) coronary arteries. The groups of patients were similar for age (left anterior descending coronary artery, 59 years; right coronary artery, 58 years; circumflex coronary artery, 62 years), patients with multivessel disease (left anterior descending coronary artery, 55%; right coronary artery, 55%; circumflex coronary artery, 64%), and patients with initial grade 0/1 antegrade flow (left anterior descending coronary artery, 79%; right coronary artery, 84%; circumflex coronary artery, 90%). Cardiogenic shock was present in eight patients with infarction of the left anterior descending coronary artery, four with infarction of the right coronary artery, and four with infarction of the circumflex coronary artery. Major catheterization laboratory events (cardioversion, cardiopulmonary resuscitation, dopamine or intra-aortic balloon pump support for hypotension, and urgent surgery) occurred in 10 patients with infarction of the left anterior descending coronary artery, eight with infarction of the right coronary artery, and four with infarction of the circumflex coronary artery (16 of 16 shock and six of 234 nonshock patients, p less than 0.001). There was one in-laboratory death (shock patient with infarction of the left anterior descending coronary artery).\n",
      "\n",
      "Label: general pathological conditions\n",
      "Text: Renal abscess in children. Three cases of renal abscesses in children are described to illustrate the variable presenting features. An additional 23 pediatric cases, reported over the past ten years, were reviewed for clinical features and therapy. Fever, loin pain, and leukocytosis were common presenting features, but less than half of all abscesses were associated with either an abnormal urinalysis or a positive urine culture. The presenting features were sometimes confused with appendicitis, peritonitis, or a Wilms tumor. An organism was identified in 17 cases--Escherichia coli in 9 children and Staphylococcus aureus in 8 children. The majority of E. coli infections occurred in girls and the majority of S. aureus infections occurred in boys. Reflux was documented in 5 patients, and 2 children had a possible extrarenal source of infection. Antibiotics alone produced a cure in 10 children (38%), but 16 children (62%) required a surgical procedure.\n",
      "\n",
      "Label: digestive system diseases\n",
      "Text: Hyperplastic polyps seen at sigmoidoscopy are markers for additional adenomas seen at colonoscopy. Asymptomatic individuals undergoing screening flexible sigmoidoscopy were prospectively studied. Polyps were found in 185 subjects. The endoscopist recorded an opinion on the polyps' histology based on endoscopic appearance. No polyps were removed at sigmoidoscopy. All subjects with rectosigmoid polyps then underwent colonoscopy and polypectomy. Of them, 99 subjects (54%) had at least one rectosigmoid adenoma, 69 (37%) had only hyperplastic polyps, and 17 (9%) had other findings. The endoscopists' opinion of the histopathology of polyps at sigmoidoscopy was correct for 61% of the lesions. Of subjects with adenomatous rectosigmoid polyps, 29% had additional adenomas at more proximal sites. Proximal adenomas were found in 28% of patients with hyperplastic rectosigmoid polyps. Patients with rectosigmoid hyperplastic polyps had the same risk for additional proximal adenomas as patients with rectosigmoid adenomatous polyps.\n",
      "\n",
      "Label: general pathological conditions\n",
      "Text: Subclavian artery to innominate vein fistula after insertion of a hemodialysis catheter. Insertion of hemodialysis catheters for temporary use is now preferentially performed by percutaneous infraclavicular subclavian vein catheterization. This method involves passage of a stiff dilator and a peel-away sheath over a guide wire, and is usually carried out without fluoroscopy. For the most part this has proved to be a valuable and safe approach. However, a small incidence of major complications occurs, which needs to be emphasized. Sixteen cases of arteriovenous fistulas between the subclavian artery or its branches and the subclavian vein have been reported so far in the literature. To date only one case of subclavian artery to innominate vein fistula has been reported. We report the second case with this complication and suggest possible preventive measures.\n",
      "\n",
      "Label: cardiovascular diseases\n",
      "Text: Effect of local inhibition of gamma-aminobutyric acid uptake in the dorsomedial hypothalamus on extracellular levels of gamma-aminobutyric acid and on stress-induced tachycardia: a study using microdialysis. Previous studies involving local microinjection of drugs that interfere with gamma-aminobutyric acid (GABA)A receptor-mediated synaptic inhibition have led to the suggestion that endogenous GABA suppresses the activity of a sympatho-excitatory mechanism in the dorsomedial hypothalamus in rats. In this study, microdialysis was used to assess and to alter pharmacologically extracellular-levels of GABA within this region while simultaneously monitoring heart rate and blood pressure. In anesthetized rats, local microdialysis for 15 min with 2.5, 10 and 40 mM nipecotic acid, an inhibitor of GABA uptake, caused concentration-related increases in GABA and taurine in the extracellular space, but no significant change in heart rate or arterial pressure. Similar perfusion with 37.5, 75 and 150 mM KCl caused concentration-related increases in GABA as well as aspartate, glutamate, taurine, glycine and alanine. Only modest, variable increases in heart rate and no effect on arterial pressure were observed during the perfusions with high potassium. In conscious rats, unilateral microdialysis of the dorsomedial hypothalamus with 0.5 mM nipecotic acid for 2 to 2.5 hr before stress coupled with contralateral microinjection of muscimol (88 pmol/250 nl) 5 min before stress significantly reduced air stress-induced tachycardia; this reduction in tachycardia was associated with markedly elevated levels of GABA in dialysates collected from the dorsomedial hypothalamus. Neither treatment alone significantly influenced stress-induced increases in heart rate, although perfusion with nipecotic acid alone evoked similar elevations in extracellular GABA. These results suggest that extracellular levels of endogenous GABA in the dorsomedial hypothalamus may regulate the cardiovascular response to stress.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Path to the processed medical text dataset which contains labels in words rather than numbers\n",
    "updated_json_file_path = '/root/llama/datasets/classification_task_new_labels.json'  \n",
    "\n",
    "# Read the updated JSON file\n",
    "with open(updated_json_file_path, 'r', encoding='utf-8') as file:\n",
    "    updated_data = json.load(file)\n",
    "\n",
    "# Let's display five first entries\n",
    "num_entries_to_display = 5\n",
    "\n",
    "# Print those entries to check the labels\n",
    "for entry in updated_data[:num_entries_to_display]:\n",
    "    print(f\"Label: {entry['label']}\")\n",
    "    print(f\"Text: {entry['clinical_conditions']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(updated_data)\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "train_testvalid_split = dataset.train_test_split(test_size=0.3)\n",
    "\n",
    "# Splitting the 30% into half for validation and test\n",
    "test_valid_split = train_testvalid_split['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "# Combining splits into a DatasetDict\n",
    "split_datasets = DatasetDict({\n",
    "    'train': train_testvalid_split['train'],\n",
    "    'validation': test_valid_split['train'],\n",
    "    'test': test_valid_split['test']\n",
    "})\n",
    "split_datasets\n",
    "train_dataset = split_datasets['train']\n",
    "validation_dataset = split_datasets['validation']\n",
    "test_dataset = split_datasets['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6833b6caca4b431e951901cf00be5f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4331 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8776a3d67a4a5ea4a0df5133641e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3032 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915f5a17b06043968b4662580d59fcce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7075 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#adding another 'text' column\n",
    "def add_text_column(example):\n",
    "    new_text = \"The following is a clinical description that corresponds to a specific medical condition. The description: '\" + example['clinical_conditions'] + \"'. Its diagnosis: '\" + example['label'] + \"'.\"\n",
    "    return {'text': new_text}\n",
    "\n",
    "# Applying the function to each dataset\n",
    "train_dataset = train_dataset.map(add_text_column)\n",
    "validation_dataset = validation_dataset.map(add_text_column)\n",
    "test_dataset = test_dataset.map(add_text_column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_parameters = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir = \"./results\"\n",
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "optim = \"paged_adamw_32bit\"\n",
    "save_steps = 100\n",
    "logging_steps = 50\n",
    "learning_rate = 2e-4\n",
    "max_grad_norm = 0.4\n",
    "#max_steps = 100\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    evaluation_strategy=\"steps\", \n",
    "    eval_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=True,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    #max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50286d40582456ebf88474311cb9f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4331 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96546d1a08b94ddeaf153be1ca2e11dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3032 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = 400\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset, \n",
    "    eval_dataset=validation_dataset,\n",
    "    peft_config=peft_parameters,\n",
    "    dataset_text_field=\"text\",  \n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_wandb = False\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using 8-bit optimizers with a version of `bitsandbytes` < 0.41.1. It is recommended to update your version as a major bug has been fixed in 8-bit optimizers.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2700' max='2700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2700/2700 2:52:29, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.434000</td>\n",
       "      <td>1.448701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.447300</td>\n",
       "      <td>1.432909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.418000</td>\n",
       "      <td>1.417161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.402800</td>\n",
       "      <td>1.414377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.406400</td>\n",
       "      <td>1.410805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.360200</td>\n",
       "      <td>1.419420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.362300</td>\n",
       "      <td>1.416409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.365400</td>\n",
       "      <td>1.413732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.309500</td>\n",
       "      <td>1.417910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.330600</td>\n",
       "      <td>1.415072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.290900</td>\n",
       "      <td>1.425231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.259000</td>\n",
       "      <td>1.433986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.271400</td>\n",
       "      <td>1.431376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.211300</td>\n",
       "      <td>1.469020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.215600</td>\n",
       "      <td>1.456787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.236200</td>\n",
       "      <td>1.454768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.128500</td>\n",
       "      <td>1.463119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.166300</td>\n",
       "      <td>1.455048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.147100</td>\n",
       "      <td>1.483469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.098100</td>\n",
       "      <td>1.522849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.108600</td>\n",
       "      <td>1.508062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.101400</td>\n",
       "      <td>1.509489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.037700</td>\n",
       "      <td>1.507018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.069300</td>\n",
       "      <td>1.495640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>1.529497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.022400</td>\n",
       "      <td>1.530656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.020800</td>\n",
       "      <td>1.519006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./results/checkpoint-100 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./results/checkpoint-200 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./results/checkpoint-300 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./results/checkpoint-400 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./results/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./results/checkpoint-600 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./results/checkpoint-700 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2700, training_loss=1.2373581610785591, metrics={'train_runtime': 10365.2236, 'train_samples_per_second': 4.178, 'train_steps_per_second': 0.26, 'total_flos': 5.4232348377931776e+17, 'train_loss': 1.2373581610785591, 'epoch': 9.97})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig.from_pretrained('outputs')\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, PreTrainedModel\n",
    "\n",
    "# We will initialize an empty list to store the prompts\n",
    "generated_summaries = []\n",
    "prompts = []\n",
    "num_descriptions_to_generate = 50\n",
    "\n",
    "# Iterating over the dataset to create prompts\n",
    "for index, data in enumerate(train_dataset):\n",
    "    if index >= num_descriptions_to_generate:\n",
    "        break\n",
    "    clinical_conditions = data['clinical_conditions']\n",
    "#    prompt = (\n",
    "#        \"Medical Diagnosis Task: You are presented with a clinical description. \"\n",
    "#        \"Based on this description, identify the most appropriate medical category for diagnosis. Respond with one of the following categories, no explanations are needed. \"\n",
    "#        \"The categories are: \\n\"\n",
    "#        \"1. Neoplasms\\n\"\n",
    "#        \"2. Digestive System Diseases\\n\"\n",
    "#        \"3. Nervous System Diseases\\n\"\n",
    "#        \"4. Cardiovascular Diseases\\n\"\n",
    "#        \"5. General Pathological Conditions\\n\\n\"\n",
    "#        f\"Clinical Description: '{clinical_conditions}'\\n\"\n",
    "#        \"Diagnosis: \"\n",
    "#    )\n",
    "    prompt = (\n",
    "    \"User: Can you help me diagnose a medical condition based on a clinical description? Please only state the diagnosis.\\n\"\n",
    "    \"Assistant: Of course. Please provide the clinical description, and I will identify the most appropriate medical category for diagnosis. Respond with one of the following categories, no explanations are needed. The categories are: \\n\"\n",
    "    \"1. Neoplasms\\n\"\n",
    "    \"2. Digestive System Diseases\\n\"\n",
    "    \"3. Nervous System Diseases\\n\"\n",
    "    \"4. Cardiovascular Diseases\\n\"\n",
    "    \"5. General Pathological Conditions\\n\\n\"\n",
    "    f\"User: Here is the clinical description: '{clinical_conditions}'.\\n\"\n",
    "    \"Assistant: The diagnosis is: \"\n",
    ")\n",
    "\n",
    "    prompts.append(prompt)\n",
    "\n",
    "# Here we generate responses using the model\n",
    "for prompt in prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=400)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Removing the prompt part from the generated text\n",
    "    prompt_end_marker = \"Assistant: The diagnosis is: \"\n",
    "    if prompt_end_marker in generated_text:\n",
    "        start_index = generated_text.find(prompt_end_marker) + len(prompt_end_marker)\n",
    "        filtered_text = generated_text[start_index:].strip()\n",
    "    else:\n",
    "        filtered_text = generated_text\n",
    "\n",
    "    generated_summaries.append(filtered_text)\n",
    "\n",
    "actual_labels = [train_dataset[i]['label'] for i in range(num_descriptions_to_generate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt at Index 6:\n",
      "User: Can you help me diagnose a medical condition based on a clinical description? Please only state the diagnosis.\n",
      "Assistant: Of course. Please provide the clinical description, and I will identify the most appropriate medical category for diagnosis. Respond with one of the following categories, no explanations are needed. The categories are: \n",
      "1. Neoplasms\n",
      "2. Digestive System Diseases\n",
      "3. Nervous System Diseases\n",
      "4. Cardiovascular Diseases\n",
      "5. General Pathological Conditions\n",
      "\n",
      "User: Here is the clinical description: 'Impact of active immunisation against enteritis necroticans in Papua New Guinea. Enteritis necroticans, known locally as pigbel, has been a major cause of illness and death among children in the highlands of Papua New Guinea. After a successful trial of active immunisation against the beta toxin of the causative organism, Clostridium perfringens type C, immunisation of children was begun in 1980. The effects of the immunisation programme on pigbel admissions in 3 of the 5 major highland hospitals were assessed. In each of the centres studied the proportion of admissions due to enteritis necroticans dropped significantly after immunisation was introduced (p less than 0.001) and hospital admissions for pigbel in 1984-86, when immunisation was well established, were less than one fifth of previous figures.'.\n",
      "Assistant: The diagnosis is: \n",
      "\n",
      "Generated Response at Index 6:\n",
      "5. General Pathological Conditions\n",
      "\n",
      "Actual Label at Index 6:\n",
      "digestive system diseases\n"
     ]
    }
   ],
   "source": [
    "display_index = 6  # for testing and viewing the results\n",
    "\n",
    "# Print the data at the chosen index\n",
    "print(\"Prompt at Index {}:\".format(display_index))\n",
    "print(prompts[display_index])\n",
    "print(\"\\nGenerated Response at Index {}:\".format(display_index))\n",
    "print(generated_summaries[display_index])\n",
    "print(\"\\nActual Label at Index {}:\".format(display_index))\n",
    "print(actual_labels[display_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.36\n",
      "Weighted Precision: 0.5772857142857143\n",
      "Weighted Recall: 0.36\n",
      "Weighted F1 Score: 0.3291230882938201\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Definiton of the regular expression pattern based on the observed responses\n",
    "pattern = r'(1|2|3|4|5|[A-Za-z\\s]+|[1-5][.: -]\\s*[A-Za-z\\s]+)'\n",
    "\n",
    "# Label mapping defining\n",
    "label_mapping = {\n",
    "    '1': 'neoplasms',\n",
    "    '2': 'digestive system diseases',\n",
    "    '3': 'nervous system diseases',\n",
    "    '4': 'cardiovascular diseases',\n",
    "    '5': 'general pathological conditions',\n",
    "    'neoplasms': 'neoplasms',\n",
    "    'digestive system diseases': 'digestive system diseases',\n",
    "    'nervous system diseases': 'nervous system diseases',\n",
    "    'cardiovascular diseases': 'cardiovascular diseases',\n",
    "    'general pathological conditions': 'general pathological conditions',\n",
    "    '1. neoplasms': 'neoplasms',\n",
    "    '2. digestive system diseases': 'digestive system diseases',\n",
    "    '3. nervous system diseases': 'nervous system diseases',\n",
    "    '4. cardiovascular diseases': 'cardiovascular diseases',\n",
    "    '5. general pathological conditions': 'general pathological conditions',\n",
    "    '1 - neoplasms': 'neoplasms',\n",
    "    '2 - digestive system diseases': 'digestive system diseases',\n",
    "    '3 - nervous system diseases': 'nervous system diseases',\n",
    "    '4 - cardiovascular diseases': 'cardiovascular diseases',\n",
    "    '5 - general pathological conditions': 'general pathological conditions',\n",
    "    '1: neoplasms': 'neoplasms',\n",
    "    '2: digestive system diseases': 'digestive system diseases',\n",
    "    '3: nervous system diseases': 'nervous system diseases',\n",
    "    '4: cardiovascular diseases': 'cardiovascular diseases',\n",
    "    '5: general pathological conditions': 'general pathological conditions',\n",
    "    '1 (Neoplasms)': 'neoplasms',\n",
    "    '2 (Digestive System Diseases)': 'digestive system diseases',\n",
    "    '3 (Nervous System Diseases)': 'nervous system diseases',\n",
    "    '4 (cardiovascular diseases)': 'cardiovascular diseases',\n",
    "    '5 (General Pathological Conditions)': 'general pathological conditions',\n",
    "}\n",
    "\n",
    "# Label keywords defining\n",
    "label_keywords = {\n",
    "    'neoplasms': ['neoplasms'],\n",
    "    'digestive system diseases': ['digestive system diseases'],\n",
    "    'nervous system diseases': ['nervous system diseases'],\n",
    "    'cardiovascular diseases': ['cardiovascular diseases'],\n",
    "    'general pathological conditions': ['general pathological conditions'],\n",
    "}\n",
    "\n",
    "# Function to find label based on keywords\n",
    "def find_label_based_on_keywords(text):\n",
    "    for label, keywords in label_keywords.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword.lower() in text.lower():\n",
    "                return label\n",
    "    return None\n",
    "\n",
    "# Extract relevant part from each output and map to actual labels\n",
    "extracted_parts = []\n",
    "\n",
    "for output in generated_summaries:\n",
    "    label_found = find_label_based_on_keywords(output)\n",
    "    if label_found:\n",
    "        extracted_parts.append(label_found)\n",
    "    else:\n",
    "        matches = re.findall(pattern, output)\n",
    "        extracted_part = None\n",
    "        for match in matches:\n",
    "            match = match.strip()\n",
    "            if match in label_mapping:\n",
    "                extracted_part = match\n",
    "                break\n",
    "        if extracted_part:\n",
    "            mapped_label = label_mapping[extracted_part]\n",
    "            extracted_parts.append(mapped_label)\n",
    "        else:\n",
    "            extracted_parts.append(\"Unknown\")\n",
    "\n",
    "# Filter out \"Unknown\" responses from actual_labels and extracted_parts\n",
    "filtered_actual_labels = [label for label, predicted in zip(actual_labels, extracted_parts) if predicted != \"Unknown\"]\n",
    "filtered_extracted_parts = [predicted for predicted in extracted_parts if predicted != \"Unknown\"]\n",
    "\n",
    "# Ensure both lists are of the same length\n",
    "if len(filtered_actual_labels) != len(filtered_extracted_parts):\n",
    "    raise ValueError(\"The number of actual labels and predicted labels must be the same.\")\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1 score\n",
    "accuracy = accuracy_score(filtered_actual_labels, filtered_extracted_parts)\n",
    "precision = precision_score(filtered_actual_labels, filtered_extracted_parts, average='weighted', labels=np.unique(filtered_extracted_parts))\n",
    "recall = recall_score(filtered_actual_labels, filtered_extracted_parts, average='weighted', labels=np.unique(filtered_extracted_parts))\n",
    "f1 = f1_score(filtered_actual_labels, filtered_extracted_parts, average='weighted', labels=np.unique(filtered_extracted_parts))\n",
    "\n",
    "# Print the calculated metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Weighted Precision:\", precision)\n",
    "print(\"Weighted Recall:\", recall)\n",
    "print(\"Weighted F1 Score:\", f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newpeft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
